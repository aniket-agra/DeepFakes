{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection using MTCNN\n",
    "\n",
    "We try to use another face detection method to see where we get optimal performance. This time we use [MTCNN](https://github.com/ipazc/mtcnn). Turns out, this works better but takes much longer than OpenCV default method. Perhaps use MTCNN only for cases where Haar Cascades fail to detect faces in a significant fraction of frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import cv2\n",
    "from mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial process is same as before. So we just copy from the other code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_dir_ofc = '/data/anaconda3/envs/aniket1/share/opencv4/haarcascades/'\n",
    "ana_dir_mac = '/Users/aniket/anaconda3/envs/aniket1/share/OpenCV/haarcascades/'\n",
    "basedir_ofc = '/data/Kaggle/DeepFake/'\n",
    "basedir_mac = '/Users/aniket/KaggleData/DeepFake/'\n",
    "\n",
    "basedir = basedir_ofc\n",
    "ana_dir = ana_dir_ofc\n",
    "#get all video filenames\n",
    "files = glob.glob(basedir+'sample_data/train_sample_videos/*.mp4')\n",
    "#read json file \n",
    "json_file = basedir+'sample_data/train_sample_videos/metadata.json'\n",
    "\n",
    "with open(json_file, 'r') as f:\n",
    "    json_dict = json.load(f)\n",
    "\n",
    "nvids = len(files)     \n",
    "vid_fname = \"\"\n",
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the loop over videos. This part is going to be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing frames from : /data/Kaggle/DeepFake/sample_data/train_sample_videos/adhsbajydo.mp4\n",
      "FAKE\n",
      "frame size = 1920.0, 1080.0, fps = 29.97\n",
      "[899, 162, 103, 131]\n",
      "[898, 160, 104, 132]\n",
      "[898, 158, 108, 137]\n",
      "[897, 156, 108, 139]\n",
      "num frames = 4\n"
     ]
    }
   ],
   "source": [
    "face_arr = []\n",
    "#loop over videos - capture image info as vectors\n",
    "for i in np.arange(10, 11) :     \n",
    "    #start capturing images \n",
    "    print('Capturing frames from : {}'.format(files[i]))\n",
    "    vid_fname = files[i].rsplit('/')[-1]\n",
    "    print(json_dict[vid_fname]['label'])\n",
    "    cap = cv2.VideoCapture(files[i]) #basedir+'sample_data/train_sample_videos/bmehkyanbj.mp4'\n",
    "    print('frame size = {}, {}, fps = {}'.format(cap.get(3), cap.get(4), cap.get(5)))\n",
    "    c = 0\n",
    "    gray0 = None\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if frame is not None : \n",
    "            c = c+1\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            ffaces = detector.detect_faces(rgb)\n",
    "            try:\n",
    "                print(ffaces[0]['box'])\n",
    "                x, y, w, h = ffaces[0]['box']\n",
    "                xl, xr, yl, yr = x, x+w, y, y+h #[int(0.95*x), int(1.05*(x+w)), int(0.8*y), int(1.1*(y+h))]\n",
    "                gray = cv2.rectangle(gray, (xl, yl), (xr, yr), (255, 0, 0), 2)\n",
    "                face_arr.append(gray[xl:xr, yl:yr])\n",
    "                if c > 1 : \n",
    "                    cv2.imshow('frame', gray)      # - gray0 + 255\n",
    "                else : \n",
    "                    cv2.imshow('frame', gray)\n",
    "                gray0 = gray.copy()\n",
    "                if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            except : \n",
    "                print('Faces not detected!')\n",
    "        else : \n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    print('num frames = {}'.format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'FAKE', 'split': 'train', 'original': 'fysyrqfguw.mp4'}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(json_dict['adhsbajydo.mp4'])\n",
    "print(len(face_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
